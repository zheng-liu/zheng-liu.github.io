[{"title":"Learning from graph-structured data","url":"/2019/07/14/graph-structured-learning/","content":"\n#### Introduction\n\nLearning from graph-structured data is a challenging and ubiquitous task. Its application domains vary from social network link prediction to molecular function group analysis, from polypharmacy side effect prediction to biological networks. This blog discusses several representation learning methods on graph-structured data. The atlas of graph representation learning methods are generally enveloped in multiple categories [1].\n\n\n\n#### Reference\n\n[1] William L. Hamilton, Rex Ying, Jure Leskovec. Representation Learning on Graphs: Methods and Applications. *IEEE Data Engineering Bulletin.* 2017. \n\n","tags":["struct","graph"]},{"title":"pu-learning medley","url":"/2019/07/14/pu-learning/","content":"\n\n\nFor classic binary classification problems, learning algorithms mostly need extended features for both positive class and negative class. Unfortunately a significant number of applications involve only positive cases. In regulatory Genomics, researchers found the positive cases through wet lab experiments and large scale post-GWAS analysis leading to extremely high confidence that a SNP (Single Nucleotide Polymorphism) is the functional genome with label as positive. However, for most of time, it is difficult to statistically proof specific genetic factor is NOT a functional one.\n\nIn order to train binary classification models, traditional learning algorithms accept the data unexplored as negative cases and train on them. Undoubtfuly, training set generated in this way is not reliable and potentially causes lots of bias for both training and classification. Thus, we have to get along with unlabeled data and try our best to classify future cases.\n\n<!-- more -->\n\n{% asset_img pn_ppn_pu.jpg pn-ppn-pu %}\n\nGenerally, the rest data (accessible data out of already-known positive cases) are merged to training set as \"unlabeled dataset\". Another preprocess frequently used is to apply an **implicit** criterion to generate \"negative\" data which actually contains both positive and negative cases in it. This filtering criterion is based on domain-specific knowledge that, there will be less positive cases in unlabeled dataset after filtering. This operation makes the unlabeled dataset more close to \"pure\" negative dataset.\n\n\n\n#### Reference\n\n[1] Elkan, C., & Noto, K. (2008). Learning classifiers from only positive and unlabeled data (pp. 213â€“9). Presented at the the 14th ACM SIGKDD international conference, New York, New York, USA: ACM Press.","tags":["semi-supervised","positive-unlabeled"]}]